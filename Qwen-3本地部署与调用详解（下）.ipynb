{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font face=\"仿宋\">课程说明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 体验课内容节选自[《2025大模型Agent智能体开发实战》(5月班)](https://ix9mq.xetslk.com/s/3u765N)完整版付费课程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;体验课时间有限，若想深度学习大模型技术，欢迎大家报名由我主讲的[《2025大模型Agent智能体开发实战》(5月班)](https://ix9mq.xetslk.com/s/3u765N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/b9ccfe4155bb650c83c4a061f3da131.jpg\" alt=\"d0c81dfe43a1becced8c07db33c3a787_\" style=\"zoom:15%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[《2025大模型Agent智能体开发实战》(5月班)](https://ix9mq.xetslk.com/s/3u765N)为【100+小时】体系大课，总共20大模块精讲精析，零基础直达大模型企业级应用！**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://wechatapppro-1252524126.cdn.xiaoeknow.com/appZe9inzwc2314/image/b_u_5ea8e780054d6_Fop5bmXf/6aueuzm7qbtmje.png?imageView2/2/q/80|imageMogr2/ignore-error/1\" alt=\"img\" style=\"zoom: 33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，5月班重磅新增DeepSeek+Agents SDK+谷歌ADK+MCP技术应用与智能体开发相关实战内容，并计划新增Qwen-3模型实战教学："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/34533e2c95414b3dc5ab1131d9d1e38.jpg\" alt=\"34533e2c95414b3dc5ab1131d9d1e38\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "部分项目成果演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **MateGen项目演示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/4.MateGen%20Pro%20%E9%A1%B9%E7%9B%AE%E5%8A%9F%E8%83%BD%E6%BC%94%E7%A4%BA.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/4.MateGen%20Pro%20%E9%A1%B9%E7%9B%AE%E5%8A%9F%E8%83%BD%E6%BC%94%E7%A4%BA.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **智能客服项目演示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E6%A1%88%E4%BE%8B%E6%BC%94%E7%A4%BA.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dify项目演示**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/2f1b47f42c65fd59e8d3a83e6cb9f13b_raw.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **LangChain&LangGraph搭建Multi-Agnet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\" controls  width=\"800\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/%E5%8F%AF%E8%A7%86%E5%8C%96%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90Multi-Agent%E6%95%88%E6%9E%9C%E6%BC%94%E7%A4%BA%E6%95%88%E6%9E%9C.mp4\", width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，若是对大模型底层原理感兴趣，也欢迎报名由我和菜菜老师共同主讲的[《2025大模型原理与实战课程》(5月班)](https://ix9mq.xetslk.com/s/49L2eN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/da5d51c998df07d747cd223c1ed25f7.jpg\" alt=\"da5d51c998df07d747cd223c1ed25f7\" style=\"zoom:20%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**两门大模型课程5月班目前上新特惠中，立减2000起，合购还有更多优惠哦~<span style=\"color:red;\">详细信息扫码添加助教，回复“大模型”，即可领取课程大纲&查看课程详情👇</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/e70ad1fd8c8b3c68b97536d74a1fc6e.jpg\" alt=\"e70ad1fd8c8b3c68b97536d74a1fc6e\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Qwen-3本地部署与调用详解（下）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ollama API本地调用Qwen3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在部署完ollama之后，即可借助ollama API（也就是OpenAI风格API）在代码环境中调用模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 导入OpenAI库及实例化OpenAI客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://192.168.110.131:11434/v1/',\n",
    "    api_key='ollama',  # 必须传递该参数\n",
    ")\n",
    "\n",
    "prompt = \"在单词\\\"strawberry\\\"中，总共有几个R？\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建消息并获得回复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "好的，我现在要解决的问题是：在单词“strawberry”中，总共有几个R？让我仔细想一想，确保自己不会犯错。\n",
      "\n",
      "首先，我需要明确目标。用户问的是“strawberry”这个单词中有多少个字母R。所以我的任务就是把这个单词正确地拆解出来，然后逐一检查每个字母是否是R，然后统计数量。\n",
      "\n",
      "接下来，我应该先写出“strawberry”这个单词的拼写，确保自己正确无误。可能有时候拼写会有错误，特别是如果不太熟悉某些单词的话。不过，我记得“strawberry”是正确的拼写，由字母s、t、r、a、w、b、e、r、r、y组成。不过，我需要再确认一遍每个字母的位置，避免出错。\n",
      "\n",
      "让我把每个字母逐个列出来：\n",
      "\n",
      "1. s\n",
      "2. t\n",
      "3. r\n",
      "4. a\n",
      "5. w\n",
      "6. b\n",
      "7. e\n",
      "8. r\n",
      "9. r\n",
      "10. y\n",
      "\n",
      "现在，我需要检查这些字母中哪些是R。看一下每个位置：\n",
      "\n",
      "- 第三个字母是r，正确。\n",
      "- 第八个字母是r。\n",
      "- 第九个字母也是r。\n",
      "- 其他位置的字母都不是r。\n",
      "\n",
      "所以看起来，这里总共有三个r。不过，等等，我是不是哪里数错了？让我再重新数一遍，确保每个位置都正确。\n",
      "\n",
      "再逐个检查：\n",
      "\n",
      "s → 不是\n",
      "\n",
      "t → 不是\n",
      "\n",
      "r → 是，第一个r（1个）\n",
      "\n",
      "a → 不是\n",
      "\n",
      "w → 不是\n",
      "\n",
      "b → 不是\n",
      "\n",
      "e → 不是\n",
      "\n",
      "r → 是，第二个r（2个）\n",
      "\n",
      "r → 是，第三个r（3个）\n",
      "\n",
      "y → 不是\n",
      "\n",
      "所以确实，有三个r。不过，我是不是哪里拼写错误？比如，是否有可能在“strawberry”中有其他位置的r？比如，可能有人会拼错，比如写成“strawberri”或者其他形式？但根据标准拼写，“strawberry”是正确的，所以应该没错。\n",
      "\n",
      "不过，可能有一些人会认为在“strawberry”中拼写是s-t-r-a-w-b-e-r-r-y，也就是三个r。是的，没错。那是不是正确的？比如常见错误可能是否多了一个r？比如，有人可能会写成s-t-r-r-a-w-b-e-r-r-y？但那样的话，字母就会重复，不过实际正确的拼写中，确实是在“berry”这个部分有三个r吗？\n",
      "\n",
      "等等，这里可能需要更仔细的检查。因为“strawberry”是由“straw”和“berry”组成。而“berry”这个词通常有三个r吗？比如，常见的是“berry”是b-e-r-r-y，对吗？这样就是两个r。那如果是这样的话，整个单词中的r数目应该是两个吗？\n",
      "\n",
      "哦，这里可能出问题了。我之前拆分的时候，可能把“berry”部分的r数错了。让我再重新拆分整个单词。\n",
      "\n",
      "“strawberry”正确的拼写是：s-t-r-a-w-b-e-r-r-y？还是s-t-r-a-w-b-e-r-y？\n",
      "\n",
      "这时候，我需要确认“strawberry”是否正确拼写为有三个r还是两个r。因为“berry”通常有两个r，对吗？比如“berry”是b-e-r-r-y，所以有两个r。那加上“straw”中的一个r（在s-t-r-a-w），所以总共有三个r？或者是不是这样？\n",
      "\n",
      "例如，如果整个单词是s-t-r-a-w-b-e-r-r-y，那么就是三个r？或者是不是s-t-r-a-w-b-e-r-y，只有两个r？\n",
      "\n",
      "这时候，我需要确认正确的英文拼写。这时候，我应该回忆或者查找正确的拼写。不过，可能有时候在记忆中会混淆，所以需要确认。\n",
      "\n",
      "举个例子，比如“strawberry”这个单词的正确拼写。例如，“strawberry”在词典中的拼写是s-t-r-a-w-b-e-r-r-y，对吗？也就是说，中间的部分是s-t-r-a-w-b-e-r-r-y，所以确实有三个r？或者是不是b-e-r-r-y，也就是两个r？\n",
      "\n",
      "此时，我可能需要更仔细地分析。比如，分解单词：\n",
      "\n",
      "strawberry 是由 straw 和 berry 组成，对吧？而“berry”这个词本身有三个r吗？不，通常“berry”是两个r，比如 blackberry（黑莓）是b-l-a-c-k-b-e-r-r-y，对吗？所以“berry”部分有三个r吗？还是两个？\n",
      "\n",
      "不，“blackberry”是b-l-a-c-k-b-e-r-r-y，所以这里的“berry”部分是b-e-r-r-y，对吗？所以这里的“berry”有三个r？或者不？不对，b-e-r-r-y，其中r出现了两次，所以是两个r。\n",
      "\n",
      "那如果是这样的话，在“strawberry”中，应该是straw + berry，对吗？\n",
      "\n",
      "那如果是这样的话，“straw”部分是s-t-r-a-w，有一个r，而“berry”是b-e-r-r-y，有两个r，所以总共有1+2=3个r？\n",
      "\n",
      "是的，这样的话，整个单词“strawberry”中的r数目是三个？\n",
      "\n",
      "但这样的话，我之前拆解的时候是正确的，总共有三个r。但是，我之前可能混淆了“berry”中的r数目？\n",
      "\n",
      "或者，可能正确的拼写是“strawberry”中有两个r？\n",
      "\n",
      "这时候，我需要再确认一次，可能自己之前对“berry”的拼写有误。\n",
      "\n",
      "例如，查字典的话，strawberry的拼写是s-t-r-a-w-b-e-r-r-y，确实是三个r。对吗？或者是不是这样？\n",
      "\n",
      "让我再想，比如，如果我拼写出“strawberry”这个单词，那么其中的“berry”部分有两个r还是三个？\n",
      "\n",
      "比如，如果我写“strawberry”，正确的拼写应该是s-t-r-a-w-b-e-r-r-y，对吗？所以这里有三个r？\n",
      "\n",
      "那这样的话，答案应该是3个r。但是，我之前可能误以为“berry”有两个r，所以可能混淆了？\n",
      "\n",
      "这时候，我需要再仔细分析一下，或者用其他方式来确认。\n",
      "\n",
      "比如，分解每个字母：\n",
      "\n",
      "s t r a w b e r r y\n",
      "\n",
      "现在，数一下每个字母：\n",
      "\n",
      "s (1), t (2), r (3), a (4), w (5), b (6), e (7), r (8), r (9), y (10)\n",
      "\n",
      "所以，r出现在第3、8、9位，共三个，对吗？\n",
      "\n",
      "是的，三个r。所以正确答案是3个r。\n",
      "\n",
      "不过，我之前可能担心是否自己对“strawberry”的拼写有误，所以需要再次确认。\n",
      "\n",
      "或者，是否存在另一种拼写？\n",
      "\n",
      "比如，是否有可能是s-t-r-a-w-b-e-r-y，即只有两个r？\n",
      "\n",
      "那这样的话，答案就是两个r？\n",
      "\n",
      "这个时候，我需要确认正确的拼写。\n",
      "\n",
      "根据我的知识，“strawberry”的正确拼写是s-t-r-a-w-b-e-r-r-y，即三个r。因为“berry”部分是b-e-r-r-y，对吗？而“straw”部分是s-t-r-a-w，所以有一个r，加上“berry”中的两个r，总共有三个r。\n",
      "\n",
      "因此，正确的答案应该是三个r。\n",
      "\n",
      "不过，可能有些人会错误地认为“berry”只有两个r，但实际上“berry”是三个r？不，不，实际上“berry”是两个r。因为“berry”是b-e-r-r-y，所以是两个r。\n",
      "\n",
      "所以，如果这样的话，整个单词“strawberry”中的r数目应该是1（来自straw） + 2（来自berry） = 3个r。对吗？是的。\n",
      "\n",
      "因此，正确的答案应该是三个。\n",
      "\n",
      "但可能有些学习者会错误地在拼写时重复r两次，导致错误地认为有三个r？或者是否正确拼写？\n",
      "\n",
      "这个时候，我需要查阅一下或者回忆正确的拼写。\n",
      "\n",
      "例如，我可以通过联想常见的拼写错误。比如，是否有人容易在“strawberry”中多写一个r？比如，正确拼写是s-t-r-a-w-b-e-r-r-y，对吗？\n",
      "\n",
      "或者，是否应该是s-t-r-a-w-b-e-r-y？即只一个r？\n",
      "\n",
      "这个时候，我可能需要再用另一种方法验证，比如分解单词的来源。\n",
      "\n",
      "“Strawberry”这个名字来源于“straw”（干草）和“berry”（浆果），所以可能拼写是straw + berry。\n",
      "\n",
      "而“berry”本身是b-e-r-r-y，对吗？比如，像“blackberry”是b-l-a-c-k-b-e-r-r-y，所以这里的“berry”部分有两个r。所以“strawberry”应该也是这样，即在“berry”中有两个r。\n",
      "\n",
      "所以，整个单词中的三个r来自于straw中的r（一个）和berry中的r（两个）：1+2=3个r。\n",
      "\n",
      "因此，结论是三个r。\n",
      "\n",
      "不过，这时候，我需要确保自己不会犯拼写错误。比如，可能有些人会把“strawberry”拼成两个r，即s-t-r-a-w-b-e-r-y，但那是错误的，正确的是三个r吗？\n",
      "\n",
      "或者，其实正确的拼写是s-t-r-a-w-b-e-r-r-y，对吗？\n",
      "\n",
      "我需要再次确认。比如，查看英式或美式拼写是否有不同。但通常来说，“strawberry”的拼写是相同的，不管英式还是美式。\n",
      "\n",
      "例如，根据牛津词典的拼写：strawberry 是 s-t-r-a-w-b-e-r-r-y，对吗？\n",
      "\n",
      "是的，这样就是三个r。所以，答案是3个。\n",
      "\n",
      "所以，我的思考过程确认了这个单词中有三个r。\n",
      "\n",
      "不过，在最开始的时候，我可能误以为“berry”中有两个r，而“straw”中有一个，所以总共有三个，正确。但可能在思考过程中我有过短暂的混淆，但最终通过仔细分析和确认，得出了正确的答案。\n",
      "\n",
      "总结一下：\n",
      "\n",
      "正确拼写为 strawberry → s-t-r-a-w-b-e-r-r-y，其中有三个r，分别在位置3、8、9。因此，答案是3个r。\n",
      "</think>\n",
      "\n",
      "在单词“strawberry”中，字母R的分布如下：\n",
      "\n",
      "1. **s**  \n",
      "2. **t**  \n",
      "3. **r**（第一个R）  \n",
      "4. **a**  \n",
      "5. **w**  \n",
      "6. **b**  \n",
      "7. **e**  \n",
      "8. **r**（第二个R）  \n",
      "9. **r**（第三个R）  \n",
      "10. **y**  \n",
      "\n",
      "通过逐个检查每个字母的位置，可以确认“strawberry”中共有 **3个R**（分别位于第3、8和9位）。因此，答案是：\n",
      "\n",
      "**3**\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='qwen3:14b',\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;因为Qwen3刚刚发布，所以Ollama目前还不支持在API调用时通过`enable_thinking`参数禁用思考过程，这里有一种策略是先用`/no_think` 提示不要进入思考过程。如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "我们来一步一步分析单词 **\"strawberry\"** 中的字母 **R** 的数量。\n",
      "\n",
      "---\n",
      "\n",
      "### 第一步：写出单词\n",
      "**strawberry**\n",
      "\n",
      "---\n",
      "\n",
      "### 第二步：逐个字母查看\n",
      "我们将单词拆分为单个字母：\n",
      "\n",
      "**s - t - r - a - w - b - e - r - r - y**\n",
      "\n",
      "---\n",
      "\n",
      "### 第三步：数一数字母 **R** 出现的次数\n",
      "从上面的字母序列中，我们可以看到 **R** 出现了 **3** 次。\n",
      "\n",
      "---\n",
      "\n",
      "### 最终答案：\n",
      "**\"strawberry\"** 中有 **3** 个 **R**。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"/no_think 在单词\\\"strawberry\\\"中，总共有几个R？\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model='qwen3:14b',\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;而此时显存占用约22G左右（14B Q4量化模型）：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291258998.png\" width=60%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、Qwen3模型接入vLLM与推理流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来继续介绍Qwen3-14B模型借助vLLM进行推理的完整流程。相比ollama，vLLM更加适合企业级高并发应用场景，但对应的，显存占用也会更高，vLLM项目主页：https://github.com/vllm-project/vllm`Vllm` 底层是基于`Pytorch` 构建，其`Gtihub` 开源地址为：https://github.com/vllm-project/vllm \n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504181137533.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从各种基准测试数据来看，同等配置下，使用 `vLLM` 框架与 `Transformer` 等传统推理库相比，其吞吐量可以提高一个数量级，这归功于以下几个特性：\n",
    "\n",
    "- **高级 GPU 优化**：利用 `CUDA` 和 `PyTorch` 最大限度地提高 `GPU` 利用率，从而实现更快的推理速度。`Ollama`其实是对`CPU-GPU`的混合应用，但`vllm`是针对纯`GPU`的优化。\n",
    "- **高级内存管理**：通过`PagedAttention`算法实现对 `KV cache`的高效管理，减少内存浪费，从而优化大模型的运行效率。\n",
    "- **批处理功能**：支持连续批处理和异步处理，从而提高多个并发请求的吞吐量。\n",
    "- **安全特性**：内置 `API` 密钥支持和适当的请求验证，不像其他完全跳过身份验证的框架。\n",
    "- **易用性**：`vLLM` 与 `HuggingFace` 模型无缝集成，支持多种流行的大型语言模型，并兼容 `OpenAI` 的 `API` 服务器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 vLLM安装与启动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;使用`vllm`框架部署`Qwen3`模型，同样需要先安装`Python`运行环境，这里我们可以复用之前安装的`Anaconda`环境，具体执行命令如下：\n",
    "\n",
    "```bash\n",
    "    conda activate qwen3\n",
    "```\n",
    "\n",
    "&emsp;&emsp;注意：`vllm`官方要求的是`Python 3.9` ~ `Python 3.12` 之间的版本，所以如果`Anaconda`环境版本不在此范围内，请务必重新创建符合要求的`Anaconda`环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504271905428.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;然后，安装`vLLM`框架，具体执行命令如下：\n",
    "\n",
    "```bash\n",
    "    pip install vllm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504271905429.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此时耐心等待安装完成即可。待安装完成后，可以使用`pip show vllm` 命令查看`vllm` 框架的安装信息，可以明确看到当前安装的`vllm` 版本号。如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504271909847.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;目前vLLM已支持Qwen3模型调用，可以在模型支持列表中查看模型关键字：https://docs.vllm.ai/en/latest/models/supported_models.html\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291124292.png\" width=60%></div>\n",
    "\n",
    "&emsp;&emsp;接下来即可按照如下流程进行调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 OpenAI风格API响应模式\n",
    "\n",
    "\n",
    "&emsp;&emsp;需要注意的是，随着模型上下文越长，所需要占用的显存也越大。根据测试，14B模型在32K上下文时，运行需要约30G显存。实际运行命令类似如下形式：\n",
    "\n",
    "```bash\n",
    "    vllm serve Qwen3-8B --max-model-len 32768 --host 192.168.110.131 --port 8000\n",
    "```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291320578.png\" width=60%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;启动vLLM时候需要谨慎的设置最大上下文和对应的运行GPU数量。\n",
    "\n",
    "- 单GPU运行命令\n",
    "\n",
    "```bash\n",
    "    cd /root/autodl-tmp\n",
    "    vllm serve ./qwen3-8B --max-model-len 32768\n",
    "```\n",
    "\n",
    "- 双GPU运行命令\n",
    "\n",
    "```bash\n",
    "    cd /root/autodl-tmp\n",
    "    CUDA_VISIBLE_DEVICES=0,1 vllm serve ./qwen3-8B --tensor-parallel-size 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;启动成功后，即可在Jupyter中进行调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 导入OpenAI库并实例化OpenAI客户端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://192.168.110.131:8000/v1\"\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建消息获得回复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"在单词\\\"strawberry\\\"中，总共有几个R？\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "嗯，用户问的是在单词“strawberry”中有几个字母R。首先，我需要确认单词的正确拼写，可能有时候会有拼写错误或者变体，但“strawberry”应该是正确的拼写。\n",
      "\n",
      "接下来，我应该把单词分解开来，逐个字母检查。先写下单词：S-T-R-A-W-B-E-R-R-Y。或者可能我记错了，应该再仔细拼一遍。S-T-R-A-W-B-E-R-R-Y？或者是不是中间有其他字母？比如，strawberry的正确拼写是S-T-R-A-W-B-E-R-R-Y，对吗？是的，没错，正确的拼写是S-T-R-A-W-B-E-R-R-Y，也就是“strawberry”。\n",
      "\n",
      "现在，我需要数一下里面有几个R。先从头开始，每个字母依次检查：\n",
      "\n",
      "S - 不是R\n",
      "T - 不是\n",
      "R - 是第一个R\n",
      "A - 不是\n",
      "W - 不是\n",
      "B - 不是\n",
      "E - 不是\n",
      "R - 第二个R\n",
      "R - 第三个R\n",
      "Y - 不是\n",
      "\n",
      "所以看起来有三个R？但可能我哪里数错了。或者是不是中间的拼写有误？\n",
      "\n",
      "或者可能用户的问题有陷阱？比如，是否在拼写中有错误？比如，正确的拼写是strawberry，其中R出现的位置是第三个字母，然后是第8个和第9个字母？或者是不是中间的拼写有误？\n",
      "\n",
      "让我再仔细拼一遍：S-T-R-A-W-B-E-R-R-Y。对的，S是第一个字母，然后T，R，A，W，B，E，R，R，Y。所以R出现在第3、8、9的位置，也就是三个R？\n",
      "\n",
      "或者是不是我数错了？比如，第三个字母是R，然后在第八个和第九个字母也是R？所以总共有三个R？\n",
      "\n",
      "但可能用户可能有不同的答案，或者我哪里出错了？\n",
      "\n",
      "或者是不是在单词中R出现两次？比如，strawberry的正确拼写是S-T-R-A-W-B-E-R-R-Y，对吗？所以第三个字母是R，然后第八和第九是R，所以三个R？\n",
      "\n",
      "或者是不是在拼写中R只出现两次？比如，可能用户认为是strawberry中R出现两次？比如，可能我记错了拼写？\n",
      "\n",
      "或者是不是应该拆分单词为straw + berry？straw是S-T-R-A-W，berry是B-E-R-R-Y，所以整个单词是strawberry，所以R在straw中的第三个字母，然后在berry中的第三个和第四个字母？所以总共有三个R？\n",
      "\n",
      "是的，这样的话，strawberry中的R出现三次。所以答案应该是三个R？\n",
      "\n",
      "不过，可能用户会误以为是两次，所以需要确认正确拼写和字母的位置。\n",
      "\n",
      "或者可能用户的问题有其他陷阱？比如，是否在单词中有其他R？或者是否在拼写中存在错误？\n",
      "\n",
      "比如，strawberry的正确拼写是S-T-R-A-W-B-E-R-R-Y，对吗？是的。所以R的位置是第三个、第八个和第九个字母，也就是三个R。因此，正确的答案应该是3个R。\n",
      "</think>\n",
      "\n",
      "在单词 \"strawberry\" 中，字母 **R** 出现了 **3 次**。具体位置如下：\n",
      "\n",
      "1. 第3个字母：**R**（strawberry → S-T-R-A-W-B-E-R-R-Y）  \n",
      "2. 第8个字母：**R**  \n",
      "3. 第9个字母：**R**  \n",
      "\n",
      "因此，总共有 **3 个 R**。\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen3:8B\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/5429b86d22f29729ef960ff48e115fe.jpg\" alt=\"5429b86d22f29729ef960ff48e115fe\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、基于llama.cpp的QwQ模型CPU推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Qwen3系列模型目前也是可以使用llama.cpp进行纯CPU推理或者CPU+GPU混合推理的。接下来介绍如何使用llama.cpp调用模型权重进行推理和对话。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. llama.cpp下载与编译\n",
    "\n",
    "- llama.cpp项目主页：https://github.com/ggml-org/llama.cpp\n",
    "\n",
    "<div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250221174445079.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于llama.cpp是个C语言项目，因此实际调用过程需要先构建项目，然后设置参数进行编译，然后最终创建可执行文件（类似于脚本），再运行本地大模型。借助llama.cpp可以实现纯CPU推理、纯GPU推理和CPU+GPU混合推理。\n",
    "\n",
    "- 依赖下载\n",
    "\n",
    "  ```bash\n",
    "  apt-get update\n",
    "  apt-get install build-essential cmake curl libcurl4-openssl-dev -y\n",
    "  ```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504281026269.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;这条命令安装了一些常用的构建和开发工具，具体的每个部分的含义如下：\n",
    "\n",
    "  - **`build-essential`**：安装一组构建必需的工具和库，包括：\n",
    "    - 编译器（如 GCC）\n",
    "    - `make` 工具\n",
    "    - 其他一些常见的构建工具，确保你的系统能进行编译。\n",
    "  - **`cmake`**：安装 **CMake** 工具，它是一个跨平台的构建系统，允许你管理项目的编译过程。\n",
    "  - **`curl`**：安装 **cURL** 工具，它是一个命令行工具，用于通过 URL 发送和接收数据。它在很多开发场景中都很有用，尤其是与网络交互时。\n",
    "  - **`libcurl4-openssl-dev`**：安装 **libcurl** 库的开发版本。它是 cURL 的一个库文件，允许你在编程中通过 cURL 发送 HTTP 请求。`libcurl4-openssl-dev` 是与 **OpenSSL** 配合使用的版本，提供了 SSL/TLS 加密支持，用于安全的 HTTP 请求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- llama.cpp源码下载\n",
    "\n",
    "&emsp;&emsp;若是AutoDL服务器，可以先开启学术加速：\n",
    "\n",
    "```bash\n",
    "    source /etc/network_turbo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果是其他服务器或者本地服务器，则可以直接进行源码下载：\n",
    "\n",
    " ```bash\n",
    "  git clone https://github.com/ggerganov/llama.cpp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;也可以直接在课件网盘中找到代码文件，直接上传服务器并解压缩："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250429200007375.png\" alt=\"image-20250429200007375\" style=\"zoom:33%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/5429b86d22f29729ef960ff48e115fe.jpg\" alt=\"5429b86d22f29729ef960ff48e115fe\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;准备好后，即可在服务器中看到llama.cpp项目文件夹：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504281026270.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;接下来，即可开始进行项目构建与编译。\n",
    "\n",
    "- 项目构建与编译\n",
    "\n",
    "  ```bash\n",
    "  cmake llama.cpp -B llama.cpp/build \\\n",
    "      -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\n",
    "  ```\n",
    "\n",
    "- **`cmake`**：运行 CMake 工具，用于配置和生成构建文件。\n",
    "\n",
    "- **`llama.cpp`**：指定项目的源代码所在的目录。在这个例子中，`llama.cpp` 是项目的根目录。\n",
    "\n",
    "- **`-B llama.cpp/build`**：指定生成构建文件的目录。`-B` 参数表示**构建目录**，`llama.cpp/build` 是生成的构建目录。这是 CMake 将生成的文件存放的地方（例如 Makefile 或 Ninja 构建文件）。\n",
    "- 同时还指定了一些编译选项：\n",
    "- **禁用共享库**（`-DBUILD_SHARED_LIBS=OFF`），生成 **静态库**。\n",
    "- **启用 CUDA 支持**（`-DGGML_CUDA=ON`），以便在有 GPU 的情况下使用 GPU 加速。\n",
    "- **启用 CURL 库支持**（`-DLLAMA_CURL=ON`），以便支持网络请求。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504281026271.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后需要进一步进行编译：\n",
    "\n",
    "  ```bash\n",
    "    cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\n",
    "  ```\n",
    "\n",
    "\n",
    "  - **`--build llama.cpp/build`**：告诉 CMake 使用 `llama.cpp/build` 目录中的构建文件来执行构建过程。这个目录是在之前运行 `cmake llama.cpp -B llama.cpp/build` 命令时生成的，包含了所有构建所需的文件（例如 Makefile 或 Ninja 构建文件）。\n",
    "  - **`--config Release`**：指定构建的配置为 **Release** 配置。\n",
    "    - **Release** 配置通常意味着启用更多的 **优化**，生成的程序运行速度较快，适合发布。\n",
    "    - 在 CMake 中，通常有两种常见的构建配置：\n",
    "      - **Debug**：用于调试版本，包含调试信息且没有做过多优化。\n",
    "      - **Release**：优化后的发布版本，去除调试信息，运行时性能更高。\n",
    "  - **`-j`**：表示并行构建，允许 CMake 使用多个 CPU 核心来加速构建过程。\n",
    "    - 如果没有指定数字，CMake 会使用默认的并行级别，通常是可用的所有 CPU 核心。你也可以指定并行的作业数，例如 `-j 8` 表示使用 8 个并行作业进行编译。\n",
    "  - **`--clean-first`**：表示在构建之前先清理掉之前的构建结果。这可以确保每次构建时都是从一个干净的状态开始，避免由于缓存或中间文件引起的编译错误。\n",
    "    - 如果你之前运行过构建并且有问题，或者希望重新构建而不使用任何缓存文件，这个选项非常有用。\n",
    "  - **`--target`**：指定构建的目标（target）。通常，一个项目会定义多个目标（比如库、可执行文件等），通过这个参数可以告诉 CMake 只编译特定的目标。\n",
    "    - **`llama-quantize`**：可能是与模型量化相关的目标。量化（quantization）是将模型的精度从浮点数降低到整数，从而减少内存占用和提高推理速度。\n",
    "    - **`llama-cli`**：可能是一个命令行工具，用于运行模型或与用户交互。\n",
    "    - **`llama-gguf-split`**：可能是一个用于拆分模型文件的目标，通常用于将一个大模型文件拆分成多个小文件，方便存储和加载。\n",
    "\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504281026272.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 复制可执行文件\n",
    "\n",
    "  ```bash\n",
    "  cp llama.cpp/build/bin/llama-* llama.cpp\n",
    "  ```\n",
    "\n",
    "  将 **所有生成的可执行文件** 从构建目录 `llama.cpp/build/bin/` 复制到项目的根目录 `llama.cpp` 下。这样可以更方便地在项目根目录下执行这些可执行文件，而无需每次都进入构建目录。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504281028231.png\" width=60%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在准备完成后，接下来即可进行调用和推理测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.借助llama.cpp运行Qwen3模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 纯CPU推理流程【1token/s】\n",
    "\n",
    "&emsp;&emsp;首先是纯CPU推理测试。此时系统只调用内存+CPU进行计算，此时不会用到GPU。\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291341031.png\" width=60%></div>\n",
    "\n",
    "&emsp;&emsp;此时模型运行门槛很低，但运行速度非常慢，只有不到1tokens/s。具体推理代码实现流程如下：\n",
    "\n",
    "```bash\n",
    "    cd ./llama.cpp\n",
    "\n",
    "    ./llama-cli --model /home/08_vllm/Qwen3-14B-GGUF/Qwen3-14B-Q4_K_M.gguf --cache-type-k q4_0 --threads 64 --prio 2 --temp 0.6 --ctx-size 512 --seed 3407 --n-gpu-layers 0 -no-cnv --prompt \"<｜User｜>你好，好久不见，请介绍下你自己。<｜Assistant｜>\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291340108.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中命令行核心参数说明：\n",
    "\n",
    "- `--threads`：CPU 核心数;\n",
    "- `--ctx-size`：输出的上下文长度；\n",
    "- `--n-gpu-layers` ：需要卸载到 GPU 的层数，设置为0时代表完全使用CPU进行推理；\n",
    "- `--temp`：模型温度参数；\n",
    "- `-no-cnv`：不进行多轮对话；\n",
    "- `--cache-type-k`：K 缓存量化为 4bit；\n",
    "- `--seed`：随机数种子；\n",
    "\n",
    "&emsp;&emsp;实际运行效果如下所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291341632.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CPU+GPU混合推理\n",
    "\n",
    "&emsp;&emsp;接下来进一步尝试CPU+GPU混合推理，我们只需要合理的设置`--n-gpu-layers`参数，即可灵活的将模型的部分层加载到GPU上进行运行。并且无需手动设置，llama.cpp会自动识别当前GPU数量以及可以分配的显存，自动将模型权重加载到各个不同的GPU上。例如，我们这里考虑将30层加载到GPU上，运行效果如下所示：\n",
    "\n",
    "```bash\n",
    "./llama-cli \\\n",
    "    --model /home/08_vllm/Qwen3-14B-GGUF/Qwen3-14B-Q4_K_M.gguf \\\n",
    "    --cache-type-k q4_0 \\\n",
    "    --threads 64 \\\n",
    "    --prio 2 \\\n",
    "    --temp 0.6 \\\n",
    "    --ctx-size 512 \\\n",
    "    --seed 3407 \\\n",
    "    --n-gpu-layers 30 \\\n",
    "    -no-cnv \\\n",
    "    --prompt \"<｜User｜>你好，好久不见，请介绍下你自己。<｜Assistant｜>\" \n",
    "```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291344892.png\" width=60%></div>\n",
    "\n",
    "&emsp;&emsp;此时显存占用不到10G：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291344893.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;推理速度略微有所提升，能达到接近2tokens/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 纯GPU推理\n",
    "\n",
    "&emsp;&emsp;最后，我们更进一步，尝试把全部的模型权重都放在GPU上进行推理。\n",
    "\n",
    "```bash\n",
    "./llama-cli \\\n",
    "    --model /home/08_vllm/Qwen3-14B-GGUF/Qwen3-14B-Q4_K_M.gguf \\\n",
    "    --cache-type-k q4_0 \\\n",
    "    --threads 64 \\\n",
    "    --prio 2 \\\n",
    "    --temp 0.6 \\\n",
    "    --ctx-size 512 \\\n",
    "    --seed 3407 \\\n",
    "    --n-gpu-layers 64 \\\n",
    "    -no-cnv \\\n",
    "    --prompt \"<｜User｜>你好，好久不见，请介绍下你自己。<｜Assistant｜>\" \n",
    "```\n",
    "\n",
    "&emsp;&emsp;此时GPU占用约11G：\n",
    "\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291347015.png\" width=60%></div>\n",
    "\n",
    "&emsp;&emsp;推理速度则能达到14tokens/s。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 六、Qwen3 接入OpenWeb-UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Open-WebUI部署流程\n",
    "\n",
    "&emsp;&emsp;首先需要安装Open-WebUI，官网地址如下：https://github.com/open-webui/open-webui。\n",
    "\n",
    "<div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214142632738.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;我们可以直接使用pip命令快速完成安装：\n",
    "\n",
    "\n",
    "```bash\n",
    "    pip install open-webui\n",
    "```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504281806967.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;可以直接使用在GitHub项目主页上直接下载完整代码包，并上传至服务器解压缩运行：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214142010929.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;此外，也可以在课件网盘中领取完整代码包，并上传至服务器解压缩运行："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250429200151840.png\" alt=\"image-20250429200151840\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/5429b86d22f29729ef960ff48e115fe.jpg\" alt=\"5429b86d22f29729ef960ff48e115fe\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在确保ollama正常运行的情况下，进行后续操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Open-WebUI启动与对话流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在准备好了Open-WebUI和一系列模型权重后，接下来我们尝试启动Open-WebUI，并借助本地模型进行问答。\n",
    "\n",
    "&emsp;&emsp;首先需要设置离线环境，避免Open-WebUI启动时自动进行模型下载：\n",
    "\n",
    "```bash\n",
    "    export HF_HUB_OFFLINE=1\n",
    "```\n",
    "\n",
    "&emsp;&emsp;然后启动Open-WebUI\n",
    "\n",
    "```bash\n",
    "    open-webui serve\n",
    "```\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504281812378.png\" width=60%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;需要注意的是，如果启动的时候仍然报错显示无法下载模型，是Open-WebUI试图从huggingface上下载embedding模型，之后我们会手动将其切换为本地运行的Embedding模型。\n",
    "\n",
    "\n",
    "<div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214154657014.png\" width=60%></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后在本地浏览器输入地址:8080端口即可访问：\n",
    "\n",
    "\n",
    "<div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214154949305.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> 若使用AutoDL，则需要使用SSH隧道工具进行地址代理\n",
    ">\n",
    "><div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214155045849.png\" width=60%></div> \n",
    "\n",
    "\n",
    "> 更多AutoDL相关操作详见公开课：《AutoDL快速入门与GPU租赁使用指南》|https://www.bilibili.com/video/BV1bxB7YYEST/\n",
    ">\n",
    "> <div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250205182609797.png\" width=60%></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后首次使用前，需要创建管理员账号：\n",
    "\n",
    "<div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214155158043.png\" width=60%></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后点击登录即可。需要注意的是，此时Open-WebUI会自动检测后台是否启动了ollama服务，并列举当前可用的模型。稍等片刻，即可进入到如下页面：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291551362.png\" width=60%></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;接下来即可进入到对话页面：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291551363.png\" width=60%></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对话效果如下所示：\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291551364.png\" width=60%></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 本地知识库检索\n",
    "\n",
    "<div align=center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214171919602.png\" width=60%></div>\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291551365.png\" width=60%></div>\n",
    "\n",
    "<div align=center><img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291551366.png\" width=60%></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 代码解释器\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291609658.png\" width=60%>\n",
    "</div>\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291609659.png\" width=60%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 调用外部工具\n",
    "\n",
    "&emsp;&emsp;Qwen3全系列支持Function calling，因此我们可以基于此完成Open-WebUI的外部工具调用工作。\n",
    "\n",
    "- Open-WebUI工具调用功能实现\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214174019551.png\" width=60%>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;然后编写天气查询函数：\n",
    "\n",
    "```python\n",
    "    import requests\n",
    "    import json\n",
    "    from fastapi import Request\n",
    "    from open_webui.models.users import Users\n",
    "\n",
    "    class Tools:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        async def get_weather(\n",
    "            self, loc: str, __request__: Request, __user__: dict, __event_emitter__=None\n",
    "        ) -> str:\n",
    "            \"\"\"\n",
    "            获取指定城市的即时天气信息。\n",
    "\n",
    "            :param loc: 城市名称（如果是非英文城市名称，请先将其翻译为英文城市名称再输入）。\n",
    "            :param __request__: HTTP 请求对象（来自 FastAPI）。\n",
    "            :param __user__: 用户信息（可以用于个性化或记录请求）。\n",
    "            :param __event_emitter__: 事件发射器，用于将实时状态更新发送到前端。\n",
    "            :return: 格式化后的天气数据，作为字符串返回。\n",
    "            \n",
    "            说明：\n",
    "            如果输入的是非英文城市名称，请先将其翻译为对应的英文城市名称再输入。\n",
    "            比如，输入“中国，北京”，需要转为 \"Beijing\"。\n",
    "            \"\"\"\n",
    "\n",
    "            # Step 1. 通知用户正在获取天气数据\n",
    "            await __event_emitter__(\n",
    "                {\n",
    "                    \"type\": \"status\",\n",
    "                    \"data\": {\"description\": f\"正在获取 {loc} 的天气数据...\", \"done\": False},\n",
    "                }\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # Step 2. 构建请求的 URL 和查询参数\n",
    "                url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "                params = {\n",
    "                    \"q\": loc,\n",
    "                    \"appid\": \"YOUR_API_KEY\",  # 你的 API 密钥\n",
    "                    \"units\": \"metric\",  # 使用摄氏度\n",
    "                    \"lang\": \"zh_cn\",  # 输出简体中文\n",
    "                }\n",
    "\n",
    "                # Step 3. 向 OpenWeather API 发送 GET 请求\n",
    "                response = requests.get(url, params=params)\n",
    "\n",
    "                # Step 4. 解析响应数据\n",
    "                data = response.json()\n",
    "\n",
    "                # Step 5. 提取并格式化天气描述\n",
    "                weather_data = data.get(\"weather\", [])\n",
    "                if weather_data:\n",
    "                    main_weather = weather_data[0].get(\"main\", \"\")\n",
    "                    description = weather_data[0].get(\"description\", \"\")\n",
    "                    weather_info = f\"当前天气：{main_weather} - {description}\"\n",
    "                else:\n",
    "                    weather_info = \"天气信息不可用。\"\n",
    "\n",
    "                # Step 6. 通知用户天气数据已经成功获取\n",
    "                await __event_emitter__(\n",
    "                    {\n",
    "                        \"type\": \"status\",\n",
    "                        \"data\": {\"description\": f\"成功获取 {loc} 的天气数据\", \"done\": True},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Step 7. 返回格式化后的天气信息并发送到前端\n",
    "                await __event_emitter__(\n",
    "                    {\n",
    "                        \"type\": \"message\",\n",
    "                        \"data\": {\"content\": f\"{loc} 的天气信息: {weather_info}\"},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # 返回格式化后的天气数据作为字符串\n",
    "                return f\"{loc} 的天气信息: {weather_info}\"\n",
    "\n",
    "            except Exception as e:\n",
    "                # Step 8. 如果发生错误，通知用户\n",
    "                await __event_emitter__(\n",
    "                    {\n",
    "                        \"type\": \"status\",\n",
    "                        \"data\": {\"description\": f\"发生错误: {str(e)}\", \"done\": True},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # 返回错误信息\n",
    "                return f\"获取天气数据时发生错误：{str(e)}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;并将函数进行写入：\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214174216021.png\" width=60%>\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;完成后即可看到新的工具：\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250214174233430.png\" width=60%>\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;在对话时可以开启天气查询函数：\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291609660.png\" width=60%>\n",
    "</div>\n",
    "\n",
    "<div align=center>\n",
    "    <img src=\"https://muyu20241105.oss-cn-beijing.aliyuncs.com/images/202504291609661.png\" width=60%>\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;至此，我们就详细介绍了QwQ模型的各类本地部署与调用方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
